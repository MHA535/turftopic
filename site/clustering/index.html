
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="An all-in-one library for topic modeling with sentence embeddings.">
      
      
      
      
        <link rel="prev" href="../GMM/">
      
      
        <link rel="next" href="../ctm/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.6">
    
    
      
        <title>Clustering Models - Turftopic</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.50c56a3b.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="#01034A" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#clustering-topic-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Turftopic" class="md-header__button md-logo" aria-label="Turftopic" data-md-component="logo">
      
  <img src="../images/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Turftopic
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Clustering Models
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/x-tabdeveloping/turftopic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Basics

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../model_overview/" class="md-tabs__link">
          
  
    
  
  Models

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../encoders/" class="md-tabs__link">
        
  
    
  
  Encoders

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Turftopic" class="md-nav__button md-logo" aria-label="Turftopic" data-md-component="logo">
      
  <img src="../images/logo.svg" alt="logo">

    </a>
    Turftopic
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/x-tabdeveloping/turftopic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basics
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    
      
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Models
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Models
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model_overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../s3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SÂ³
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../KeyNMF/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KeyNMF
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../GMM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GMM
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Clustering Models
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Clustering Models
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#the-model" class="md-nav__link">
    <span class="md-ellipsis">
      The Model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comparison-to-bertopic-and-top2vec" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison to BERTopic and Top2Vec
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Considerations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#api-reference" class="md-nav__link">
    <span class="md-ellipsis">
      API Reference
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ctm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Autoencoding Models
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../encoders/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Encoders
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="clustering-topic-models">Clustering Topic Models</h1>
<p>Clustering topic models conceptualize topic modeling as a clustering task.
Essentially a topic for these models is a tightly packed group of documents in semantic space.</p>
<p>The first contextually sensitive clustering topic model was introduced with Top2Vec, and BERTopic has also iterated on this idea.</p>
<p>Turftopic contains flexible implementations of these models where you have control over each of the steps in the process,
while sticking to a minimal amount of extra dependencies.
While the models themselves can be equivalent to BERTopic and Top2Vec implementations, Turftopic might not offer some of the implementation-specific features,
that the other libraries boast.</p>
<h2 id="the-model">The Model</h2>
<h3 id="1-dimensionality-reduction">1. Dimensionality Reduction</h3>
<p>It is common practice in clustering topic modeling literature to reduce the dimensionality of the embeddings before clustering them.
This is to avoid the curse of dimensionality, an issue, which many clustering models are affected by.</p>
<p>Dimensionality reduction by default is done with scikit-learn's TSNE implementation in Turftopic,
but users are free to specify the model that will be used for dimensionality reduction.</p>
<p>Our knowledge about the impacts of choice of dimensionality reduction is limited, and has not yet been explored in the literature.
Top2Vec and BERTopic both use UMAP, which has a number of desirable properties over alternatives (arranging data points into cluster-like structures, better preservation of global structure than TSNE, speed).</p>
<h3 id="2-clustering">2. Clustering</h3>
<p>After reducing the dimensionality of the embeddings, they are clustered with a clustering model.
As HDBSCAN  has only been part of scikit-learn since version 1.3.0, Turftopic uses OPTICS as its default.</p>
<p>Some clustering models are capable of discovering the number of clusters in the data.
This is a useful and yet-to-be challenged property of clustering topic models.</p>
<p>Practice suggests, however, that in large corpora, this frequently results in a very large number of topics, which is impractical for interpretation.
Models' hyperparameters can be adjusted to account for this behaviour, but the impact of choice of hyperparameters on topic quality is more or less unknown.</p>
<h3 id="3a-term-importance-proximity-to-cluster-centroids">3a. Term Importance: Proximity to Cluster Centroids</h3>
<p>Clustering topic models rely on post-hoc term importance estimation.
Currently there are two methods used for this.</p>
<p>The solution introduced in Top2Vec (Angelov, 2020) is that of estimating terms' importances for a given topic from their
embeddings' cosine similarity to the centroid of the embeddings in a cluster.</p>
<figure>
  <img src="https://raw.githubusercontent.com/ddangelov/Top2Vec/master/images/topic_words.svg?sanitize=true" width="600px" style="margin-left: auto;margin-right: auto;">
  <figcaption>Terms Close to the Topic Vector <br>(figure from Top2Vec documentation)</figcaption>
</figure>

<p>This has three implications:</p>
<ol>
<li>Topic descriptions are very specific. As the closest terms to the topic vector are selected, they tend to also be very close to each other.
 The issue with this is that many of the documents in a topic might not get proper coverage.</li>
<li>It is assumed that the clusters are convex and spherical. This might not at all be the case, and especially when clusters are concave, 
 the closest terms to the centroid might end up describing a different, or nonexistent topic.
 In other words: The mean might not be a representative datapoint of the population.</li>
<li>Noise rarely gets into topic descriptions. Since functions words or contaminating terms are not very likely to be closest to the topic vector,
 decriptions are typically clean.</li>
</ol>
<figure>
  <img src="../images/cluster_centroid_problem.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Centroids of Non-Convex Clusters</figcaption>
</figure>

<h3 id="3b-term-importance-c-tf-idf">3b. Term Importance: c-TF-IDF</h3>
<p>The solution to this issue, suggested by Grootendorst (2022) to this issue was c-TF-IDF.</p>
<p>c-TF-IDF is a weighting scheme based on the number of occurrences of terms in each cluster.
Terms which frequently occur in other clusters are inversely weighted so that words, which are specific to a topic gain larger importance.</p>
<p>Let <span class="arithmatex">\(X\)</span> be the document term matrix where each element (<span class="arithmatex">\(X_{ij}\)</span>) corresponds with the number of times word <span class="arithmatex">\(j\)</span> occurs in a document <span class="arithmatex">\(i\)</span>.
Turftopic uses a modified version of c-TF-IDF, which is calculated in the following manner:</p>
<ul>
<li>Estimate weight of term <span class="arithmatex">\(j\)</span> for topic <span class="arithmatex">\(z\)</span>: <br>
<span class="arithmatex">\(tf_{zj} = \frac{t_{zj}}{w_z}\)</span>, where 
<span class="arithmatex">\(t_{zj} = \sum_{i \in z} X_{ij}\)</span> is the number of occurrences of a word in a topic and 
<span class="arithmatex">\(w_{z}= \sum_{j} t_{zj}\)</span> is all words in the topic <br></li>
<li>Estimate inverse document/topic frequency for term <span class="arithmatex">\(j\)</span>:<br />
<span class="arithmatex">\(idf_j = log(\frac{N}{\sum_z |t_{zj}|})\)</span>, where
<span class="arithmatex">\(N\)</span> is the total number of documents.</li>
<li>Calculate importance of term <span class="arithmatex">\(j\)</span> for topic <span class="arithmatex">\(z\)</span>: <br />
<span class="arithmatex">\(c-TF-IDF{zj} = tf_{zj} \cdot idf_j\)</span></li>
</ul>
<p>This solution is generally to be preferred to centroid-based term importance (and the default in Turftopic), as it is more likely to give correct results.
On the other hand, c-TF-IDF can be sensitive to words with atypical statistical properties (stop words), and can result in low diversity between topics, when clusters are joined post-hoc.</p>
<h2 id="comparison-to-bertopic-and-top2vec">Comparison to BERTopic and Top2Vec</h2>
<p>Turftopic's implementation differs in multiple places to BERTopic and Top2Vec.
You can, however, construct models in Turftopic that imitate the behaviour of these other packages.</p>
<p>The main differences to these packages are:
 - The c-TF-IDF formulae are not identical. BERTopic's version might be added in the future for compatibility.
 - Dimensionality reduction in BERTopic and Top2Vec is done with UMAP.
 - Clustering is in BERTopic and Top2Vec is done with HDBSCAN.
 - Turftopic does not include many of the visualization and model-specific utilities that BERTopic does.</p>
<p>To get closest to the functionality of the two other packages you can manually set the clustering and dimensionality reduction model when creating the models:</p>
<p>You will need UMAP and scikit-learn&gt;=1.3.0:</p>
<div class="highlight"><pre><span></span><code>pip<span class="w"> </span>install<span class="w"> </span>umap-learn<span class="w"> </span>scikit-learn&gt;<span class="o">=</span><span class="m">1</span>.3.0
</code></pre></div>
<p>This is how you build a BERTopic-like model in Turftopic:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">turftopic</span> <span class="kn">import</span> <span class="n">ClusteringTopicModel</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">HDBSCAN</span>
<span class="kn">import</span> <span class="nn">umap</span>

<span class="c1"># I also included the default parameters of BERTopic so that the behaviour is as</span>
<span class="c1"># close as possible</span>
<span class="n">berttopic</span> <span class="o">=</span> <span class="n">ClusteringTopicModel</span><span class="p">(</span>
    <span class="n">dimensionality_reduction</span><span class="o">=</span><span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span>
        <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">clustering</span><span class="o">=</span><span class="n">HDBSCAN</span><span class="p">(</span>
        <span class="n">min_cluster_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
        <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
        <span class="n">cluster_selection_method</span><span class="o">=</span><span class="s2">&quot;eom&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">feature_importance</span><span class="o">=</span><span class="s2">&quot;ctfidf&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<p>This is how you build a Top2Vec model in Turftopic:</p>
<div class="highlight"><pre><span></span><code><span class="n">top2vec</span> <span class="o">=</span> <span class="n">ClusteringTopicModel</span><span class="p">(</span>
    <span class="n">dimensionality_reduction</span><span class="o">=</span><span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span>
        <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
        <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span>
    <span class="p">),</span>
    <span class="n">clustering</span><span class="o">=</span><span class="n">HDBSCAN</span><span class="p">(</span>
        <span class="n">min_cluster_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
        <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
        <span class="n">cluster_selection_method</span><span class="o">=</span><span class="s2">&quot;eom&quot;</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">feature_importance</span><span class="o">=</span><span class="s2">&quot;centroid&quot;</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div>
<h2 id="considerations">Considerations</h2>
<h3 id="strengths">Strengths</h3>
<ul>
<li>Automatic Discovery of Number of Topics: Clustering models can find the number of topics by themselves. This is a useful quality of these models as practicioners can rarely make an informed decision about the number of topics a-priori.</li>
<li>No Assumptions of Normality: With clustering models you can avoid making assumptions about cluster shapes. This is in contrast with GMMs, which assume topics to be Gaussian components.</li>
<li>Outlier Detection: OPTICS, HDBSCAN or DBSCAN contain outlier detection. This way, outliers do not influence topic representations.</li>
<li>Not Affected by Embedding Size: Since the models include dimensionality reduction, they are not as influenced by the curse of dimensionality as other methods.</li>
</ul>
<h3 id="weaknesses">Weaknesses</h3>
<ul>
<li>Scalability: Clustering models typically cannot be fitted in an online fashion, and manifold learning is usually inefficient in large corpora. When the number of texts is huge, the number of topics also gets inflated, which is impractical for interpretation.</li>
<li>Lack of Nuance: The models are unable to capture multiple topics in a document or capture uncertainty around topic labels. This makes the models impractical for longer texts as well.</li>
<li>Sensitivity to Hyperparameters: While do not have to set the number of topics directly, the hyperparameters you choose has a huge impact on the number of topics you will end up getting. (see figure)</li>
<li>Transductivity: Some clustering methods are transductive, meaning you can't predict topical content for new documents, as they would change the cluster structure.</li>
</ul>
<figure>
  <img src="../images/umap_hdbscan_stability.png" width="80%" style="margin-left: auto;margin-right: auto;">
  <figcaption>Effect of UMAP's and HDBSCAN's Hyperparameters on the Number of Topics in 20 Newsgroups</figcaption>
</figure>

<h2 id="api-reference">API Reference</h2>


<div class="doc doc-object doc-class">



<h3 id="turftopic.models.cluster.ClusteringTopicModel" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>          <code>turftopic.models.cluster.ClusteringTopicModel</code>


</h3>


  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="turftopic.base.ContextualModel" href="../model_overview/#turftopic.base.ContextualModel">ContextualModel</a></code>, <code><span title="sklearn.base.ClusterMixin">ClusterMixin</span></code></p>

  
      <p>Topic models, which assume topics to be clusters of documents
in semantic space.
Models also include a dimensionality reduction step to aid clustering.</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">turftopic</span> <span class="kn">import</span> <span class="n">KeyNMF</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">HDBSCAN</span>
<span class="kn">import</span> <span class="nn">umap</span>

<span class="n">corpus</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;some text&quot;</span><span class="p">,</span> <span class="s2">&quot;more text&quot;</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>

<span class="c1"># Construct a Top2Vec-like model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ClusteringTopicModel</span><span class="p">(</span>
    <span class="n">dimensionality_reduction</span><span class="o">=</span><span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span>
    <span class="n">clustering</span><span class="o">=</span><span class="n">HDBSCAN</span><span class="p">(),</span>
    <span class="n">feature_importance</span><span class="o">=</span><span class="s2">&quot;centroid&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_topics</span><span class="p">()</span>
</code></pre></div>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>encoder</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="turftopic.base.Encoder">Encoder</span>, str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p>
            </div>
          </td>
          <td>
                <code>&#39;sentence-transformers/all-MiniLM-L6-v2&#39;</code>
          </td>
        </tr>
        <tr>
          <td><code>vectorizer</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sklearn.feature_extraction.text.CountVectorizer">CountVectorizer</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Vectorizer used for term extraction.
Can be used to prune or filter the vocabulary.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>dimensionality_reduction</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sklearn.base.TransformerMixin">TransformerMixin</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dimensionality reduction step to run before clustering.
Defaults to TSNE with cosine distance.
To imitate the behavior of BERTopic or Top2Vec you should use UMAP.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>clustering</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="sklearn.base.ClusterMixin">ClusterMixin</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Clustering method to use for finding topics.
Defaults to OPTICS with 25 minimum cluster size.
To imitate the behavior of BERTopic or Top2Vec you should use HDBSCAN.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>feature_importance</code></td>
          <td>
                <code><span title="typing.Literal">Literal</span>[&#39;ctfidf&#39;, &#39;centroid&#39;]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Method for estimating term importances.
'centroid' uses distances from cluster centroid similarly
to Top2Vec.
'ctfidf' uses BERTopic's c-tf-idf.</p>
            </div>
          </td>
          <td>
                <code>&#39;ctfidf&#39;</code>
          </td>
        </tr>
    </tbody>
  </table>

            <details class="quote">
              <summary>Source code in <code>turftopic/models/cluster.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">ClusteringTopicModel</span><span class="p">(</span><span class="n">ContextualModel</span><span class="p">,</span> <span class="n">ClusterMixin</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Topic models, which assume topics to be clusters of documents</span>
<span class="sd">    in semantic space.</span>
<span class="sd">    Models also include a dimensionality reduction step to aid clustering.</span>

<span class="sd">    ```python</span>
<span class="sd">    from turftopic import KeyNMF</span>
<span class="sd">    from sklearn.cluster import HDBSCAN</span>
<span class="sd">    import umap</span>

<span class="sd">    corpus: list[str] = [&quot;some text&quot;, &quot;more text&quot;, ...]</span>

<span class="sd">    # Construct a Top2Vec-like model</span>
<span class="sd">    model = ClusteringTopicModel(</span>
<span class="sd">        dimensionality_reduction=umap.UMAP(5),</span>
<span class="sd">        clustering=HDBSCAN(),</span>
<span class="sd">        feature_importance=&quot;centroid&quot;</span>
<span class="sd">    ).fit(corpus)</span>
<span class="sd">    model.print_topics()</span>
<span class="sd">    ```</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    encoder: str or SentenceTransformer</span>
<span class="sd">        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</span>
<span class="sd">    vectorizer: CountVectorizer, default None</span>
<span class="sd">        Vectorizer used for term extraction.</span>
<span class="sd">        Can be used to prune or filter the vocabulary.</span>
<span class="sd">    dimensionality_reduction: TransformerMixin, default None</span>
<span class="sd">        Dimensionality reduction step to run before clustering.</span>
<span class="sd">        Defaults to TSNE with cosine distance.</span>
<span class="sd">        To imitate the behavior of BERTopic or Top2Vec you should use UMAP.</span>
<span class="sd">    clustering: ClusterMixin, default None</span>
<span class="sd">        Clustering method to use for finding topics.</span>
<span class="sd">        Defaults to OPTICS with 25 minimum cluster size.</span>
<span class="sd">        To imitate the behavior of BERTopic or Top2Vec you should use HDBSCAN.</span>
<span class="sd">    feature_importance: &#39;ctfidf&#39; or &#39;centroid&#39;, default &#39;ctfidf&#39;</span>
<span class="sd">        Method for estimating term importances.</span>
<span class="sd">        &#39;centroid&#39; uses distances from cluster centroid similarly</span>
<span class="sd">        to Top2Vec.</span>
<span class="sd">        &#39;ctfidf&#39; uses BERTopic&#39;s c-tf-idf.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoder</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span>
            <span class="n">Encoder</span><span class="p">,</span> <span class="nb">str</span>
        <span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">,</span>
        <span class="n">vectorizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CountVectorizer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dimensionality_reduction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TransformerMixin</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clustering</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ClusterMixin</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">feature_importance</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;ctfidf&quot;</span><span class="p">,</span> <span class="s2">&quot;centroid&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;ctfidf&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">integer_message</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">encoder</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoder_</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="k">if</span> <span class="n">vectorizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">default_vectorizer</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">vectorizer</span>
        <span class="k">if</span> <span class="n">clustering</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span> <span class="o">=</span> <span class="n">OPTICS</span><span class="p">(</span><span class="n">min_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span> <span class="o">=</span> <span class="n">clustering</span>
        <span class="k">if</span> <span class="n">dimensionality_reduction</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dimensionality_reduction</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span>
                <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;cosine&quot;</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dimensionality_reduction</span> <span class="o">=</span> <span class="n">dimensionality_reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_importance</span> <span class="o">=</span> <span class="n">feature_importance</span>

    <span class="k">def</span> <span class="nf">fit_predict</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Fits model and predicts cluster labels for all given documents.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        raw_documents: iterable of str</span>
<span class="sd">            Documents to fit the model on.</span>
<span class="sd">        y: None</span>
<span class="sd">            Ignored, exists for sklearn compatibility.</span>
<span class="sd">        embeddings: ndarray of shape (n_documents, n_dimensions), optional</span>
<span class="sd">            Precomputed document encodings.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ndarray of shape (n_documents)</span>
<span class="sd">            Cluster label for all documents (-1 for outliers)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">console</span> <span class="o">=</span> <span class="n">Console</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">console</span><span class="o">.</span><span class="n">status</span><span class="p">(</span><span class="s2">&quot;Fitting model&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">status</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Encoding documents&quot;</span><span class="p">)</span>
                <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
                <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Encoding done.&quot;</span><span class="p">)</span>
            <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Extracting terms&quot;</span><span class="p">)</span>
            <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
            <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Term extraction done.&quot;</span><span class="p">)</span>
            <span class="n">vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
            <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Reducing Dimensionality&quot;</span><span class="p">)</span>
            <span class="n">reduced_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensionality_reduction</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span>
                <span class="n">embeddings</span>
            <span class="p">)</span>
            <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Dimensionality reduction done.&quot;</span><span class="p">)</span>
            <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Clustering documents&quot;</span><span class="p">)</span>
            <span class="n">cluster_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">reduced_embeddings</span><span class="p">)</span>
            <span class="n">clusters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">cluster_labels</span><span class="p">)</span>
            <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Clustering done.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>
            <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Estimating term importances&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_importance</span> <span class="o">==</span> <span class="s2">&quot;ctfidf&quot;</span><span class="p">:</span>
                <span class="n">document_topic_matrix</span> <span class="o">=</span> <span class="n">label_binarize</span><span class="p">(</span>
                    <span class="n">cluster_labels</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">soft_ctf_idf</span><span class="p">(</span><span class="n">document_topic_matrix</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Encoding vocabulary&quot;</span><span class="p">)</span>
                <span class="n">vocab_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">cluster_centroid_distance</span><span class="p">(</span>
                    <span class="n">cluster_labels</span><span class="p">,</span>
                    <span class="n">embeddings</span><span class="p">,</span>
                    <span class="n">vocab_embeddings</span><span class="p">,</span>
                    <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels_</span> <span class="o">=</span> <span class="n">cluster_labels</span>
            <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Model fitting done.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cluster_labels</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">label_binarize</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h4 id="turftopic.models.cluster.ClusteringTopicModel.fit_predict" class="doc doc-heading">
<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>          <code class="highlight language-python"><span class="n">fit_predict</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>Fits model and predicts cluster labels for all given documents.</p>



  <p><strong>Parameters:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td><code>raw_documents</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Documents to fit the model on.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr>
          <td><code>y</code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Ignored, exists for sklearn compatibility.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr>
          <td><code>embeddings</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Precomputed document encodings.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><strong>Returns:</strong></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr>
          <td>
                <code>ndarray of shape (n_documents)</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Cluster label for all documents (-1 for outliers)</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>turftopic/models/cluster.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit_predict</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">raw_documents</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fits model and predicts cluster labels for all given documents.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    raw_documents: iterable of str</span>
<span class="sd">        Documents to fit the model on.</span>
<span class="sd">    y: None</span>
<span class="sd">        Ignored, exists for sklearn compatibility.</span>
<span class="sd">    embeddings: ndarray of shape (n_documents, n_dimensions), optional</span>
<span class="sd">        Precomputed document encodings.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ndarray of shape (n_documents)</span>
<span class="sd">        Cluster label for all documents (-1 for outliers)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">console</span> <span class="o">=</span> <span class="n">Console</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">console</span><span class="o">.</span><span class="n">status</span><span class="p">(</span><span class="s2">&quot;Fitting model&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">status</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Encoding documents&quot;</span><span class="p">)</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
            <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Encoding done.&quot;</span><span class="p">)</span>
        <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Extracting terms&quot;</span><span class="p">)</span>
        <span class="n">doc_term_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
        <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Term extraction done.&quot;</span><span class="p">)</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
        <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Reducing Dimensionality&quot;</span><span class="p">)</span>
        <span class="n">reduced_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimensionality_reduction</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span>
            <span class="n">embeddings</span>
        <span class="p">)</span>
        <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Dimensionality reduction done.&quot;</span><span class="p">)</span>
        <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Clustering documents&quot;</span><span class="p">)</span>
        <span class="n">cluster_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clustering</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">reduced_embeddings</span><span class="p">)</span>
        <span class="n">clusters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">cluster_labels</span><span class="p">)</span>
        <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Clustering done.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span>
        <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Estimating term importances&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_importance</span> <span class="o">==</span> <span class="s2">&quot;ctfidf&quot;</span><span class="p">:</span>
            <span class="n">document_topic_matrix</span> <span class="o">=</span> <span class="n">label_binarize</span><span class="p">(</span>
                <span class="n">cluster_labels</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">soft_ctf_idf</span><span class="p">(</span><span class="n">document_topic_matrix</span><span class="p">,</span> <span class="n">doc_term_matrix</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">status</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;Encoding vocabulary&quot;</span><span class="p">)</span>
            <span class="n">vocab_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder_</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">components_</span> <span class="o">=</span> <span class="n">cluster_centroid_distance</span><span class="p">(</span>
                <span class="n">cluster_labels</span><span class="p">,</span>
                <span class="n">embeddings</span><span class="p">,</span>
                <span class="n">vocab_embeddings</span><span class="p">,</span>
                <span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels_</span> <span class="o">=</span> <span class="n">cluster_labels</span>
        <span class="n">console</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Model fitting done.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cluster_labels</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["toc.integrate", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "toc.follow", "content.code.copy"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.e1c3ead8.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>