# Semantic Signal Separation ($S^3$)

Semantic Signal Separation tries to recover dimensions/axes along which most of the semantic variations can be explained.
A topic in S³ is an axis of semantics in the corpus.
This makes the model able to recover more nuanced topical content in documents, but is not optimal when you expect topics to be groupings of documents.

<figure>
  <img src="../images/s3_math_correct.png" width="60%" style="margin-left: auto;margin-right: auto;">
  <figcaption> Schematic overview of S³  </figcaption>
</figure>

## The Model

### 0. Encoding

Documents in $S^3$ get first encoded using an [encoder model](encoders.md).

- Let the encodings of documents in the corpus be $X$.

### 1. Decomposition

The next step is to decompose the embedding matrix using ICA, this step discovers the underlying semantics axes as latent independent components in the embeddings.

- Decompose $X$ using FastICA: $X = AS$, where $A$ is the mixing matrix and $S$ is the document-topic-matrix.

### 2. Term Importance Estimation

Term importances for each topic are calculated by encoding the entire vocabulary of the corpus using the same embedding model,
then recovering the strength of each latent component in the word embedding matrix.
The strength of the components in the words will be interpreted as the words' importance in a given topic.

- Let the matrix of word encodings be $V$.
- Calculate the pseudo-inverse of the mixing matrix $C = A^{+}$, where $C$ is the _unmixing matrix_.
- Estimate component strength by multiplying word encodings with the unmixing matrix: $W = VC^T$. $W^T$ is then the topic-term matrix (`model.components_`).

## Comparison to Classical Models

$S^3$ is potentially the closest you can get with contextually sensitive models to classical matrix decomposition approaches, such as NMF or Latent Semantic Analysis.
The conceptualization is very similar to these models, but instead of recovering factors of word use, $S^3$ recovers dimensions in a continuous semantic space.
This means that you get many of the advantages of those models, including incredible speed, low sensitivity to hyperparameters and stable results.

Most of the intuitions you have about LSA will also apply with $S^3$, but it might give more surprising results, as embedding models can potentially learn different efficient representations of semantics from humans.

$S^3$ is also way more robust to stop words, meaning that you won't have to do extensive preprocessing.

## Interpretation

### Negative terms

Terms, which rank lowest on a topic have meaning in $S^3$.
Whenever interpreting semantic axes, you should probably consider both ends of the axis.
As such, when you print or export topics from $S^3$, the lowest ranking terms will also be shown along with the highest ranking ones.

Here's an example on ArXiv ML papers:

```python
from turftopic import SemanticSignalSeparation
from sklearn.feature_extraction.text import CountVectorizer

model = SemanticSignalSeparation(5, vectorizer=CountVectorizer(), random_state=42)
model.fit(corpus)

model.print_topics(top_k=5)
```

|   | **Positive**                                                     | **Negative**                                              |
|---|------------------------------------------------------------------|-----------------------------------------------------------|
| 0 | clustering, histograms, clusterings, histogram, classifying      | reinforcement, exploration, planning, tactics, reinforce  |
| 1 | textual, pagerank, litigants, marginalizing, entailment          | matlab, waveforms, microcontroller, accelerometers, microcontrollers |
| 2 | sparsestmax, denoiseing, denoising, minimizers, minimizes        | automation, affective, chatbots, questionnaire, attitudes  |
| 3 | rebmigraph, subgraph, subgraphs, graphsage, graph                | adversarial, adversarially, adversarialization, adversary, security |
| 4 | clustering, estimations, algorithm, dbscan, estimation           | cnn, deepmind, deeplabv3, convnet, deepseenet              |


### Concept Compass

If you want to gain a deeper understanding of terms' relation to axes, you can produce a *concept compass*.
This involves plotting terms in a corpus along two semantic axes.

In order to use the compass in Turftopic you will need to have `plotly` installed:

```bash
pip install plotly
```

You can display a compass based on a fitted model like so:

```python
fig = model.concept_compass(topic_x=1, topic_y=4)
fig.show()
```

<figure>
  <img src="../images/arxiv_ml_compass.png" width="60%" style="margin-left: auto;margin-right: auto;">
  <figcaption> Concept Compass of ArXiv ML Papers along two semantic axes. </figcaption>
</figure>



## Considerations

### Strengths

 - Nuanced Content: Documents are assumed to contain multiple topics and the model can therefore work on corpora where texts are longer and might not group in semantic space based on topic.
 - Efficiency: FastICA is called fast for a reason. S³ is one of the most computationally efficient models in Turftopic.
 - Novel Descriptions: S³ tends to discover topics that no other models do. This is due to its interpretation of what a topic is.
 - High Quality: Topic descriptions tend to be high quality and easily interpretable.

### Weaknesses

 - Sometimes Unintuitive: Neural embedding models might have a different mapping of the semantic space than humans. Sometimes S³ uncovers unintuitive dimensions of meaning as a result of this.
 - Moderate Scalability: The model cannot be fitted in an online fashion. It is reasonably scalable, but for very large corpora you might want to consider using a different model.

## API Reference

::: turftopic.models.decomp.SemanticSignalSeparation
